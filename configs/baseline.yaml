# Baseline video captioning configuration
# ResNet-50 (vision encoder) + LSTM (decoder)
# Command: python -m video_captioning.commands train_baseline

# Include base config
defaults:
  - dataset/msr_vtt
  - model/baseline
  - training/trainer
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none
  - _self_

# Override only what differs from dataset defaults
dataset:
  tokenizer:
    model_dir: artifacts/tokenizers

# Inference/Generation configuration
# Common settings apply to all generation modes.
generation:
  max_length: 100  # Maximum caption length
  temperature: 1.0  # Sampling temperature (1.0 = no change)

  # Beam search parameters
  beam_search_settings:
    beam_size: 5
    length_penalty: 1.2

  # Top-k sampling parameters
  topk_settings:
    k: 30

# Validation-time generation/metrics can be much slower than training.
# Keep it configurable and default to a fast setup.
validation:
  compute_metrics: true
  # Limit how many validation samples we actually generate captions for.
  # Loss is still computed on all val batches.
  max_samples: 1024
  # Generation during validation: "greedy" is much faster than beam.
  generation_mode: topk  # greedy | beam | topk
  # If true, uses generation.beam_size/max_length/temperature/length_penalty.
  # If false, disables caption generation + metrics entirely (loss-only val).
  generate_captions: true
  # METEOR (NLTK+WordNet) is comparatively slow; enable only when needed.
  compute_bleu_4: true
  compute_meteor: true

# Logging configuration
logging:
  level: INFO
  output_dir: artifacts
  use_mlflow: true
  mlflow_uri: http://127.0.0.1:8080
  experiment_name: baseline
  tracking_uri: http://127.0.0.1:8080
  log_every_n_steps: 10
